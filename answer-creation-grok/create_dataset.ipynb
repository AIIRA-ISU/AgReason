{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the jsonl for the gork ag reasoning 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 297 tasks in:\n",
      "  /work/mech-ai-scratch/zare/Prompt_creation/answer-creation-grok/data/grok-3-beta/grok-3-beta_processed_questions.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def load_processed_questions(file_path):\n",
    "    \"\"\"\n",
    "    Reads the annotation JSON file and returns a list of all 'processed_question' strings.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return [entry['processed_question'] for entry in data]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # === CONFIGURABLE PARAMETERS ===\n",
    "    model_name = \"grok-3-beta\"  # ← change this to target any other model\n",
    "    annotation_file = (\n",
    "        \"/work/mech-ai-scratch/zare/Prompt_creation/answer-creation-grok/\"\n",
    "        \"data/final_annotation_qa.json\"\n",
    "    )\n",
    "    output_base = \"/work/mech-ai-scratch/zare/Prompt_creation/answer-creation-grok/data\"\n",
    "    # ===============================\n",
    "\n",
    "    # Load questions\n",
    "    questions = load_processed_questions(annotation_file)\n",
    "\n",
    "    # Build tasks list\n",
    "    tasks = []\n",
    "    for idx, q in enumerate(questions, start=1):\n",
    "        tasks.append({\n",
    "            \"model\": model_name,\n",
    "            \"metadata\": {\"question_index\": idx},\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are an expert agronomy assistant. Provide clear, complete, \"\n",
    "                        \"and actionable advice on cover crop and abiotic stress recommendations.\"\n",
    "                    )\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Please answer the following question:\\n\\n{q}\"\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    # Prepare output directory and file\n",
    "    output_dir = os.path.join(output_base, model_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, f\"{model_name}_processed_questions.jsonl\")\n",
    "\n",
    "    # Write all tasks to a single JSONL\n",
    "    with open(output_file, 'w') as out_f:\n",
    "        for task in tasks:\n",
    "            out_f.write(json.dumps(task) + \"\\n\")\n",
    "\n",
    "    print(f\"Created {len(tasks)} tasks in:\\n  {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "model_name = \"grok-3-beta\"\n",
    "def clean_results(jsonl_path, cleaned_path):\n",
    "    cleaned = []\n",
    "    with open(jsonl_path, 'r') as infile:\n",
    "        for line in infile:\n",
    "            # parse the three‐element array\n",
    "            request_json, response_json, metadata = json.loads(line)\n",
    "\n",
    "            # find the user message\n",
    "            user_msg = next(m for m in request_json[\"messages\"] if m[\"role\"] == \"user\")\n",
    "            raw = user_msg[\"content\"]\n",
    "\n",
    "            # extract the question text\n",
    "            if isinstance(raw, str):\n",
    "                question = raw\n",
    "            else:\n",
    "                question = \" \".join(\n",
    "                    item.get(\"text\", \"\")\n",
    "                    for item in raw\n",
    "                    if isinstance(item, dict) and item.get(\"type\") == \"text\"\n",
    "                )\n",
    "\n",
    "            # remove the unwanted prefix if present\n",
    "            prefix = \"Please answer the following question:\\n\\n\"\n",
    "            if question.startswith(prefix):\n",
    "                question = question[len(prefix):]\n",
    "\n",
    "            # extract the assistant answer\n",
    "            answer = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "            # response ID and any metadata\n",
    "            resp_id = response_json.get(\"id\")\n",
    "            q_idx   = metadata.get(\"question_index\")\n",
    "\n",
    "            cleaned.append({\n",
    "                \"question_index\": q_idx,\n",
    "                \"response_id\":   resp_id,\n",
    "                \"question\":      question,\n",
    "                \"answer\":        answer\n",
    "            })\n",
    "\n",
    "    # write out the cleaned JSON\n",
    "    with open(cleaned_path, 'w', encoding='utf-8') as out:\n",
    "        json.dump(cleaned, out, indent=2, ensure_ascii=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_results(\n",
    "        jsonl_path=f\"/work/mech-ai-scratch/zare/Prompt_creation/answer-creation-grok/data/{model_name}/{model_name}_processed_results.jsonl\",\n",
    "        cleaned_path=f\"/work/mech-ai-scratch/zare/Prompt_creation/answer-creation-grok/data/{model_name}/{model_name}_cleaned_responses.json\"\n",
    "    )\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the respons of the ground truth and the respons of one llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the annotation file\n",
    "with open('/work/mech-ai-scratch/zare/Prompt_creation/answer-creation-grok/final_annotation_qa.json', 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Load the cleaned responses file\n",
    "with open('/work/mech-ai-scratch/zare/Prompt_creation/answer-creation-grok/cleaned_responses.json', 'r') as f:\n",
    "    cleaned = json.load(f)\n",
    "\n",
    "# Map questions to cleaned answers\n",
    "cleaned_map = {entry['question']: entry['answer'] for entry in cleaned}\n",
    "\n",
    "# Find matching entries\n",
    "matches = []\n",
    "for ann in annotations:\n",
    "    pq = ann.get('processed_question')\n",
    "    ann_answer = ann.get('answer')\n",
    "    if pq in cleaned_map:\n",
    "        matches.append({\n",
    "            'Question': pq,\n",
    "            'DeepseekR1 Answer': ann_answer,\n",
    "            'Grok 2 Answer': cleaned_map[pq]\n",
    "        })\n",
    "\n",
    "# Create DataFrame and print it\n",
    "df = pd.DataFrame(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/work/mech-ai-scratch/zare/Prompt_creation/answer-creation-grok/matched_answers.csv', index=False, header=['Question', 'DeepseekR1 Answer', 'Grok 2 Answer']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
